{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import os, glob, skimage, dataset, cv2\n",
    "from skimage import io, transform\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "from dataset import download\n",
    "from tools import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Download PINES Dataset]: 100%|██████████| 5067/5067 [4:02:31<00:00,  3.77s/it]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dataset.download at 0x7fc22c2cb278>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset 다운로드\n",
    "download(\"./S1_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = explore_dir('./dataset')\n",
    "\n",
    "# csv파일에서 train과 test 인덱스 찾기\n",
    "train_indices = list()\n",
    "test_indices = list()\n",
    "\n",
    "csv_file = pd.read_csv(\"./S1_Data.csv\")\n",
    "\n",
    "for i in range(len(list(csv_file.Holdout))):\n",
    "    if csv_file.Holdout[i]=='Train':\n",
    "        train_indices.append(i)\n",
    "    else:\n",
    "        test_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test 인덱스를 이용하여 train csv와 test csv 생성 \n",
    "csv_file = csv_file.as_matrix()\n",
    "\n",
    "train_csv = csv_file[train_indices[0]][np.newaxis,:]\n",
    "test_csv = csv_file[test_indices[0]][np.newaxis,:]\n",
    "\n",
    "for i, entity in enumerate(csv_file):\n",
    "    if i in train_indices[1:] :\n",
    "        train_csv = np.concatenate((train_csv,[entity]),axis=0)\n",
    "    elif i in test_indices[1:] :\n",
    "        test_csv = np.concatenate((test_csv,[entity]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 서로 다른 image들의 크기를 각 축의 평균값으로 조정\n",
    "shape_list = list(nib.load(path).get_data().shape for path in img_path[1])\n",
    "mean_shape = list(sum(shape_list[i][j] for i in range(len(shape_list)))//len(shape_list) for j in {0,1,2})\n",
    "\n",
    "train_image_shape = list()\n",
    "test_image_shape = list()\n",
    "\n",
    "for i, nibFilename in enumerate(img_path[0]):\n",
    "    try : \n",
    "        list(train_csv[:,7]).index(nibFilename)\n",
    "        train_image_shape.append(nib.load(img_path[1][i]).get_data().shape)\n",
    "    except : \n",
    "        test_image_shape.append(nib.load(img_path[1][i]).get_data().shape)\n",
    "\n",
    "# train_csv와 test_csv에 속하는 각 image들의 전체 크기를 저장\n",
    "train_num_shape = list(sum(train_image_shape[i][j] for i in range(len(train_image_shape))) for j in {0,1,2})\n",
    "test_num_shape = list(sum(test_image_shape[i][j] for i in range(len(test_image_shape))) for j in {0,1,2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "train_count = 0\n",
    "test_count = 0\n",
    "train_x_batches = list()\n",
    "train_y_batches = list()\n",
    "train_z_batches = list()\n",
    "test_x_batches = list()\n",
    "test_y_batches = list()\n",
    "test_z_batches = list()\n",
    "    \n",
    "train_x_labels = list()\n",
    "train_y_labels = list()\n",
    "train_z_labels = list()\n",
    "test_x_labels = list()\n",
    "test_y_labels = list()\n",
    "test_y_labels = list()\n",
    "\n",
    "train_x_labels = list()\n",
    "train_y_labels = list()\n",
    "train_z_labels = list()\n",
    "test_x_labels = list()\n",
    "test_y_labels = list()\n",
    "test_z_labels = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_load:   0%|          | 0/5067 [00:00<?, ?it/s]/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "image_load: 100%|██████████| 5067/5067 [41:52<00:00,  2.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(img_path[0])) as pbar:\n",
    "    pbar.set_description('image_load')\n",
    "    for i, nibFilename in enumerate(img_path[0]):\n",
    "        img = np.array(nib.load(img_path[1][i]).get_data(), dtype=np.float64)\n",
    "        \n",
    "        # 주어진 각각의 축을 기준으로 2차원 행렬의 크기 (단일 채널 이미지) 재조정\n",
    "        x_img = skimage.transform.resize(img,(img.shape[0],mean_shape[1],mean_shape[2]))\n",
    "        y_img = skimage.transform.resize(img.transpose(1,0,2),(img.shape[1],mean_shape[0],mean_shape[2]))\n",
    "        z_img = skimage.transform.resize(img.transpose(2,0,1),(img.shape[2],mean_shape[0],mean_shape[1]))\n",
    "        \n",
    "        \n",
    "        if nibFilename in train_csv[:,7]:\n",
    "            \"\"\"\n",
    "            100개 단위로 이미지 데이터 호출 및 병합 후 100개 초과시 외부 리스트에 저장한 뒤에\n",
    "            다시 100개 단위로 이미지 데이터 호출 뒤 병합 진행. (메모리 고려) - train,test 동일\n",
    "            \"\"\"\n",
    "            train_x_dataset = x_img if train_count==0 else np.concatenate((train_x_dataset,x_img), axis=0)\n",
    "            train_y_dataset = y_img if train_count==0 else np.concatenate((train_y_dataset,y_img), axis=0)\n",
    "            train_z_dataset = z_img if train_count==0 else np.concatenate((train_z_dataset,z_img), axis=0)\n",
    "            \n",
    "            if train_count==100:\n",
    "                train_x_batches.append(copy(train_x_dataset))\n",
    "                train_y_batches.append(copy(train_y_dataset))\n",
    "                train_z_batches.append(copy(train_z_dataset))\n",
    "                train_count=0\n",
    "            else:\n",
    "                train_count+=1\n",
    "                \n",
    "            # 각 축에 해당하는 img 크기와 해당 image의 Rating(between 1 and 5)을 labeling - train\n",
    "            idx=list(train_csv[:,7]).index(nibFilename)\n",
    "            for idx in range(img.shape[0]):\n",
    "                train_x_labels.append(train_csv[idx][10]) \n",
    "            for idx in range(img.shape[1]):\n",
    "                train_y_labels.append(train_csv[idx][10])\n",
    "            for idx in range(img.shape[2]):\n",
    "                train_z_labels.append(train_csv[idx][10])\n",
    "            \n",
    "        elif nibFilename in test_csv[:,7]:\n",
    "            test_x_dataset = x_img if test_count==0 else np.concatenate((test_x_dataset,x_img), axis=0)\n",
    "            test_y_dataset = y_img if test_count==0 else np.concatenate((test_y_dataset,y_img), axis=0)\n",
    "            test_z_dataset = z_img if test_count==0 else np.concatenate((test_z_dataset,z_img), axis=0)\n",
    "            \n",
    "            if test_count==100:\n",
    "                test_x_batches.append(copy(test_x_dataset))\n",
    "                test_y_batches.append(copy(test_y_dataset))\n",
    "                test_z_batches.append(copy(test_z_dataset))\n",
    "                test_count=0\n",
    "            else:\n",
    "                test_count+=1\n",
    "            \n",
    "            # 각 축에 해당하는 img 크기와 해당 image의 Rating(between 1 and 5)을 labeling - test\n",
    "            idx=list(test_csv[:,7]).index(nibFilename)\n",
    "            for idx in range(img.shape[0]):\n",
    "                test_x_labels.append(test_csv[idx][10])    \n",
    "            for idx in range(img.shape[1]):\n",
    "                test_y_labels.append(test_csv[idx][10])\n",
    "            for idx in range(img.shape[2]):\n",
    "                test_z_labels.append(test_csv[idx][10])\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "    # 모든 과정이 끝난 후 남은 이미지 데이터 append    \n",
    "    train_x_batches.append(train_x_dataset)\n",
    "    train_y_batches.append(train_y_dataset)\n",
    "    train_z_batches.append(train_z_dataset)\n",
    "    \n",
    "    test_x_batches.append(test_x_dataset)\n",
    "    test_y_batches.append(test_y_dataset)\n",
    "    test_z_batches.append(test_z_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100개씩 나눠진 이미지 데이터들을 하나의 dataset으로 합침\n",
    "\n",
    "train_x_dataset = np.concatenate((train_x_batches), axis=0)\n",
    "train_y_dataset = np.concatenate((train_y_batches), axis=0)\n",
    "train_z_dataset = np.concatenate((train_z_batches), axis=0)\n",
    "\n",
    "test_x_dataset = np.concatenate((test_x_batches), axis=0)\n",
    "test_y_dataset = np.concatenate((test_y_batches), axis=0)\n",
    "test_z_dataset = np.concatenate((test_z_batches), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Processed Images: 100%|██████████| 1278056/1278056 [01:43<00:00, 12364.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# 처리한 이미지를 저장\n",
    "with tqdm(total=len(train_x_dataset)+len(train_y_dataset)+len(train_z_dataset)+len(test_x_dataset)+len(test_y_dataset)+len(test_z_dataset)) as pbar:\n",
    "    pbar.set_description('Saving Processed Images')\n",
    "    for i, img in enumerate(train_x_dataset):\n",
    "        cv2.imwrite('./train_dataset/x/img_'+str(i+1)+'.jpg',img)\n",
    "        pbar.update(1)\n",
    "    for i, img in enumerate(train_y_dataset):\n",
    "        cv2.imwrite('./train_dataset/y/img_'+str(i+1)+'.jpg',img)\n",
    "        pbar.update(1)\n",
    "    for i, img in enumerate(train_z_dataset):\n",
    "        cv2.imwrite('./train_dataset/z/img_'+str(i+1)+'.jpg',img)\n",
    "        pbar.update(1)\n",
    "        \n",
    "    for i, img in enumerate(test_x_dataset):\n",
    "        cv2.imwrite('./test_dataset/x/img_'+str(i+1)+'.jpg',img)\n",
    "        pbar.update(1)\n",
    "    for i, img in enumerate(test_y_dataset):\n",
    "        cv2.imwrite('./test_dataset/y/img_'+str(i+1)+'.jpg',img)\n",
    "        pbar.update(1)\n",
    "    for i, img in enumerate(test_z_dataset):\n",
    "        cv2.imwrite('./test_dataset/z/img_'+str(i+1)+'.jpg',img)\n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
